{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9987b34f",
   "metadata": {},
   "source": [
    "# Session 3: Review, Quiz & Key Takeaways\n",
    "\n",
    "In this final session, we'll review what we've learned about static and contextual embeddings, answer your questions, and reinforce concepts through an optional quiz and summary.\n",
    "\n",
    "Let's reflect on what we've covered and test your understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a5fcd",
   "metadata": {},
   "source": [
    "## üß† Review Questions\n",
    "\n",
    "Try answering these before checking the answers:\n",
    "\n",
    "1. What is the key difference between static and contextual word embeddings?\n",
    "2. Name two models that generate static word embeddings.\n",
    "3. Which model introduced the use of transformers in NLP?\n",
    "4. Why might contextual embeddings outperform static embeddings in tasks like Named Entity Recognition?\n",
    "5. Can you think of a word that changes meaning based on context?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b1a61",
   "metadata": {},
   "source": [
    "## ‚úÖ Answers\n",
    "\n",
    "1. **Static embeddings assign one vector per word regardless of context, while contextual embeddings vary the vector based on sentence context.**\n",
    "2. **Word2Vec and GloVe.**\n",
    "3. **BERT (Bidirectional Encoder Representations from Transformers).**\n",
    "4. **Because they can disambiguate word meanings based on surrounding words, capturing more accurate semantics.**\n",
    "5. **Examples: 'bat' (animal or sports equipment), 'bank' (finance or river), 'spring' (season or coil).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73bab6",
   "metadata": {},
   "source": [
    "## üéÆ Quick Quiz (Multiple Choice)\n",
    "\n",
    "**Q1:** Which model uses co-occurrence statistics to generate embeddings?\n",
    "- A) Word2Vec\n",
    "- B) BERT\n",
    "- C) GloVe\n",
    "- D) GPT-3\n",
    "\n",
    "**Q2:** Which of these produces different embeddings for a word depending on the sentence?\n",
    "- A) Word2Vec\n",
    "- B) GloVe\n",
    "- C) FastText\n",
    "- D) BERT\n",
    "\n",
    "**Q3:** What type of architecture does BERT use?\n",
    "- A) LSTM\n",
    "- B) Transformer\n",
    "- C) CNN\n",
    "- D) RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cb6ca",
   "metadata": {},
   "source": [
    "## üìù Quiz Answers\n",
    "\n",
    "**Q1:** C) GloVe\n",
    "\n",
    "**Q2:** D) BERT\n",
    "\n",
    "**Q3:** B) Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008f8cf-a521-4523-bb63-4e85b6211589",
   "metadata": {},
   "source": [
    "# Practical Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01051e97-6d44-4a20-8f08-04b7e9ab3c09",
   "metadata": {},
   "source": [
    "Compute embeddings for the following sentence pairs using both models\n",
    "\n",
    "**Pair 1:**\n",
    "\n",
    "\"The bank charged me fees.\" (financial institution)\n",
    "\n",
    "\"We fished near the river bank.\" (river edge)\n",
    "\n",
    "**Pair 2:**\n",
    "\n",
    "\"The pitcher threw a ball.\" (baseball player)\n",
    "\n",
    "\"She poured water from the pitcher.\" (container)\n",
    "\n",
    "Calculate cosine similarity for each pair with both Word2Vec and BERT.\n",
    "\n",
    "Answer the questions:\n",
    "\n",
    "1. Which model gave higher similarity for ambiguous words (e.g., \"bank\", \"pitcher\")? Why?\n",
    "\n",
    "2. When would Word2Vec be sufficient? When would you need BERT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f808dc",
   "metadata": {},
   "source": [
    "## üìå Key Takeaways\n",
    "\n",
    "- **Word Embeddings** are vector representations of words.\n",
    "- **Static Embeddings** (Word2Vec, GloVe): Each word = one fixed vector, context not considered.\n",
    "- **Contextual Embeddings** (ELMo, BERT): Word vectors change depending on surrounding context.\n",
    "- **BERT** uses a Transformer architecture and is pre-trained using masked language modeling.\n",
    "- **Applications** include semantic similarity, text classification, Q&A, and more.\n",
    "\n",
    "Congratulations on completing the tutorial series! üéâ\n",
    "You're now equipped with both theoretical and practical knowledge of word embeddings in NLP.\n",
    "Feel free to explore further with Hugging Face Transformers and other models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
